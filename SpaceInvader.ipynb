{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e5267",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ale-py gym gym[Atari] tensorflow matplotlib pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5936442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a0ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v4', render_mode='human')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f9637",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = env.observation_space.shape\n",
    "action_shape = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e8873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Hyperparameters\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "REPLAY_SIZE = 2000\n",
    "EPISODES = 500\n",
    "TARGET_MODEL_UPDATE = 200\n",
    "REPLAY_MEMORY = 50_000\n",
    "\n",
    "# CNN model params\n",
    "LEARNING_RATE = 0.00025\n",
    "KERNEL_SIZE = [8,4,3]\n",
    "STRIDES = [4,2,1]\n",
    "POOL_SIZE = 2\n",
    "\n",
    "# Q-learning params\n",
    "Q_LEARNING_RATE = 0.7\n",
    "DISCOUNT_FACTOR = 0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe360d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Model - Create a convolutional neural network with Keras\n",
    "\n",
    "def network(state_shape, action_shape):\n",
    "    \n",
    "    #     initializer = keras.initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='truncated_normal')\n",
    "    #     initializer = keras.initializers.HeUniform()\n",
    "    initializer = keras.initializers.GlorotUniform()\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(keras.layers.Conv2D(32, kernel_size=KERNEL_SIZE[0], input_shape=(4,85,80,1), activation='relu', \n",
    "        padding='same', strides=STRIDES[0], kernel_initializer=initializer))\n",
    "    #model.add(keras.layers.MaxPooling2D(pool_size=POOL_SIZE))\n",
    "\n",
    "    # Hidden convolutional layers\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=KERNEL_SIZE[1], activation='relu', padding='same', \n",
    "        strides=STRIDES[1], kernel_initializer=initializer))\n",
    "    #model.add(keras.layers.MaxPooling2D(pool_size=POOL_SIZE))\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=KERNEL_SIZE[2], activation='relu', padding='same', \n",
    "        strides=STRIDES[2], kernel_initializer=initializer))\n",
    "    #model.add(keras.layers.MaxPooling2D(pool_size=POOL_SIZE))\n",
    "\n",
    "    # Flatten and use fully connected network\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(512, activation='relu', kernel_initializer=initializer))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(keras.layers.Dense(action_shape, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.MeanSquaredError(), optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE), metrics=[keras.metrics.MeanSquaredError()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fa8d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = network(state_shape, action_shape)\n",
    "target_model = network(state_shape, action_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Preprocessing\n",
    "\n",
    "def preprocess(obs, normalize=False):\n",
    "    # Crop out score and floor\n",
    "    img = obs[25:195]  \n",
    "\n",
    "    # Downsize\n",
    "    img = img[::2, ::2]\n",
    "\n",
    "    # Take greyscale (black and white)\n",
    "    img = img.mean(axis=2)  \n",
    "\n",
    "    # color = np.array([210, 164, 74]).mean()\n",
    "    # img[img==color] = 0  \n",
    "    # img[img==144] = 0\n",
    "    # img[img==109] = 0\n",
    "    img[img != 0] = 1\n",
    "\n",
    "    # Is this needed? normalize the image from -1 to +1  \n",
    "    # No difference visually but tensor is different\n",
    "    if normalize:\n",
    "        img = (img - 128) / 128 - 1  \n",
    "\n",
    "    # print(\"before: \", obs.shape)\n",
    "    # print(\"after: \", img.shape)\n",
    "\n",
    "    # reshape to 1D tensor\n",
    "    return img.reshape(85,80,1)\n",
    "\n",
    "# frame stacking\n",
    "# https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\n",
    "# https://arxiv.org/pdf/1312.5602.pdf\n",
    "# need to get overlapping sets of frames\n",
    "# Ex: X1, X2, ... , X7 -> [X1, X2, X3, X4], [X2, X3, X4, X5], ... , [X4, X5, X6, X7]\n",
    "\n",
    "frame_skip = 4 # only one every four screenshot is considered. If there is no subsampling, not enough information to discern motion\n",
    "frame_stack_size = 4\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new):\n",
    "    frame = preprocess(state)\n",
    "    if is_new: # new episode\n",
    "        # replace stacked_frames with 4 copies of current frame\n",
    "        for i in range(frame_stack_size):\n",
    "            stacked_frames.append(frame)\n",
    "    else:\n",
    "        # take elementwise maxima of newest frame in stacked_frames and frame\n",
    "        stacked_frames.append(np.maximum(stacked_frames[3],frame))\n",
    "    stacked_state = np.stack(stacked_frames)\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f24b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Training agent\n",
    "\n",
    "def train(env, replay_memory, model, target_model, epoch):\n",
    "\n",
    "    if len(replay_memory) <= REPLAY_SIZE:\n",
    "        return\n",
    "\n",
    "    batch = random.sample(replay_memory, BATCH_SIZE)\n",
    "    states = np.array([step[0] for step in batch])\n",
    "    q_values = model.predict(states)\n",
    "    succesive_states = np.array([step[3] for step in batch])\n",
    "    succesive_q_values = target_model.predict(succesive_states)\n",
    "\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "\n",
    "    for i, (state, action, reward, new_state, dead) in enumerate(batch):\n",
    "        if not dead:\n",
    "            # Bellman Equation : r(s) + gamma * max_a'(Q(s',a'))\n",
    "            qValue = reward + DISCOUNT_FACTOR * np.max(succesive_q_values[i])\n",
    "        else:\n",
    "            # Pick reward as the episode has ended; no succesive state\n",
    "            qValue = -1\n",
    "        \n",
    "        # TODO: Figure out y_train values work or not\n",
    "        # Temporal Difference\n",
    "        # q_value_arr for a state s : [qVal action1, qval action1, ..., qval action18] \n",
    "        q_value_arr = q_values[i]\n",
    "        # Qvalue for action a  : Q(s,a) + alpha(r(s) + gamma*max_a'(Q(s',a')) - Q(s, a))         \n",
    "        # q_value_arr[action] = (1 - Q_LEARNING_RATE) * q_value_arr[action] + Q_LEARNING_RATE * qValue\n",
    "        q_value_arr[action] = qValue # q_value_arr[action] - LEARNING_RATE * (qValue - q_value_arr[action])\n",
    "\n",
    "        X_train.append(state)\n",
    "        Y_train.append(q_value_arr)\n",
    "    \n",
    "    if epoch % 250 == 0:\n",
    "        checkpoint_filepath = \"./tmp/cp.ckpt\"\n",
    "\n",
    "        model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            save_weights_only=True,\n",
    "            save_best_only=True,\n",
    "            verbose=1,\n",
    "            save_freq=1,\n",
    "            monitor='mean_squared_error',\n",
    "            mode='min')\n",
    "\n",
    "        # Model weights are saved if it's the best seen so far.\n",
    "        model.fit(np.array(X_train), np.array(Y_train), batch_size=BATCH_SIZE, callbacks=[model_checkpoint_callback]) \n",
    "    else:\n",
    "        model.fit(np.array(X_train), np.array(Y_train), batch_size=BATCH_SIZE)\n",
    "\n",
    "#     # The model weights (that are considered the best) are loaded into the model.\n",
    "#     model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253a18e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Deep Q-Learning agent\n",
    "\n",
    "epsilon = 1\n",
    "eps_min = 0.05\n",
    "eps_max = 1\n",
    "decay = 0.015\n",
    "\n",
    "model = network(state_shape, action_shape)\n",
    "target_model = network(state_shape, action_shape)\n",
    "target_model.set_weights(model.get_weights())\n",
    "\n",
    "# initialize with zeroes\n",
    "stacked_frames = deque(maxlen = frame_stack_size)\n",
    "for i in range(frame_stack_size):\n",
    "    stacked_frames.append([np.zeros((85,80), dtype=int)])\n",
    "\n",
    "# Memory buffer to store the last N experiences\n",
    "replay_memory = deque(maxlen=REPLAY_MEMORY)\n",
    "\n",
    "update_target_counter = 0\n",
    "step_counter = 0\n",
    "epoch = 0\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    score = 0 \n",
    "    done = False\n",
    "    dead = False\n",
    "    start_life = 3\n",
    "    \n",
    "    state,_,_,_ = env.step(0)\n",
    "    stacked_state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "    while not done:\n",
    "        step_counter += 1\n",
    "        dead = False\n",
    "\n",
    "        # Epsilon Greedy Strategy with explore probability epsilon\n",
    "        if np.random.rand() <= epsilon:\n",
    "            # Explore \n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Exploit best action from cnn\n",
    "            predictions = model.predict(np.array([stacked_state,])).flatten()\n",
    "            action = np.argmax(predictions)\n",
    "\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        new_stacked_state, stacked_frames = stack_frames(stacked_frames, new_state, False)\n",
    "        \n",
    "        if start_life > info['lives']:\n",
    "            dead = True\n",
    "            start_life = info['lives']\n",
    "\n",
    "        replay_memory.append([stacked_state, action, reward, new_stacked_state, dead])\n",
    "\n",
    "        if step_counter % 4 == 0 or done:\n",
    "            epoch += 1\n",
    "            train(env, replay_memory, model, target_model, epoch)\n",
    "\n",
    "        score += reward\n",
    "        stacked_state = new_stacked_state\n",
    "        state = new_state\n",
    "\n",
    "        if update_target_counter >= TARGET_MODEL_UPDATE:\n",
    "                update_target_counter = 0\n",
    "                target_model.set_weights(model.get_weights())\n",
    "\n",
    "        if done:\n",
    "              print('Score: {} after epsidoe = {}'.format(score, episode))\n",
    "\n",
    "    # Exponential Decay for epsilon (explore with atleast eps_min probability)\n",
    "    epsilon = eps_min + (eps_max - eps_min) * np.exp(-decay * episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c645c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0854b060",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.save(\"models/model#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33ea498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    env = gym.make('SpaceInvaders-v4, render_mode='human')\n",
    "    state = env.reset()\n",
    "\n",
    "    TEST_EPISODES = 100\n",
    "\n",
    "    model = keras.models.load_model(\"models/model#\")\n",
    "    scores = []\n",
    "    \n",
    "    # initialize with zeroes\n",
    "    stacked_frames = deque(maxlen = frame_stack_size)\n",
    "    for i in range(frame_stack_size):\n",
    "        stacked_frames.append([np.zeros((85,80), dtype=int)])\n",
    "\n",
    "    for episode in range(TEST_EPISODES):\n",
    "        state = env.reset()\n",
    "        score = 0 \n",
    "        done = False\n",
    "\n",
    "        stacked_state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "        while not done:\n",
    "            predictions = model.predict(np.array([stacked_state,])).flatten()\n",
    "            action = np.argmax(predictions)\n",
    "\n",
    "\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            new_stacked_state, stacked_frames = stack_frames(stacked_frames, new_state, False)\n",
    "\n",
    "            score += reward\n",
    "            stacked_state = new_stacked_state\n",
    "            state = new_state\n",
    "\n",
    "            if done:\n",
    "                scores.append(score)\n",
    "                print('episode: {}, score: {}'.format(episode, score))\n",
    "\n",
    "    \n",
    "    x = np.array([i for i in range(TEST_EPISODES)])\n",
    "    y = np.array(scores)\n",
    "\n",
    "    print(np.average(y))\n",
    "    \n",
    "    plt.plot(x, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59a89a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136afc25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
