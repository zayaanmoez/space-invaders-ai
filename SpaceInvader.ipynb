{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c358f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ale-py gym gym[Atari] tensorflow matplotlib pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d177d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ALE/SpaceInvaders-v5', render_mode='human')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = env.observation_space.shape\n",
    "action_shape = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846e4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Hyperparameters\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "REPLAY_SIZE = 2000\n",
    "EPISODES = 300\n",
    "TARGET_MODEL_UPDATE = 200\n",
    "REPLAY_MEMORY = 50_000\n",
    "\n",
    "# CNN model params\n",
    "LEARNING_RATE = 0.003\n",
    "KERNEL_SIZE = [8,4,3]\n",
    "STRIDES = [4,2,1]\n",
    "POOL_SIZE = 2\n",
    "\n",
    "# Q-learning params\n",
    "Q_LEARNING_RATE = 0.2\n",
    "DISCOUNT_FACTOR = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0b1176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Model - Create a convolutional neural network with Keras\n",
    "\n",
    "def network(state_shape, action_shape):\n",
    "    \n",
    "    initializer = keras.initializers.HeUniform()\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(keras.layers.Conv2D(32, kernel_size=KERNEL_SIZE[0], input_shape=(85,80,1), activation='relu', \n",
    "        padding='same', strides=STRIDES[0], kernel_initializer=initializer))\n",
    "    #model.add(keras.layers.MaxPooling2D(pool_size=POOL_SIZE))\n",
    "\n",
    "    # Hidden convolutional layers\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=KERNEL_SIZE[1], activation='relu', padding='same', \n",
    "        strides=STRIDES[0], kernel_initializer=initializer))\n",
    "    #model.add(keras.layers.MaxPooling2D(pool_size=POOL_SIZE))\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=KERNEL_SIZE[2], activation='relu', padding='same', \n",
    "        strides=STRIDES[0], kernel_initializer=initializer))\n",
    "    #model.add(keras.layers.MaxPooling2D(pool_size=POOL_SIZE))\n",
    "\n",
    "    # Flatten and use fully connected network\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(512, activation='relu', kernel_initializer=initializer))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(keras.layers.Dense(action_shape, activation='softmax', kernel_initializer=initializer))\n",
    "\n",
    "    model.compile(loss=keras.losses.Huber(), optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1d3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = network(state_shape, action_shape)\n",
    "target_model = network(state_shape, action_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f6d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Preprocessing\n",
    "\n",
    "def preprocess(obs, normalize=False):\n",
    "    # Crop out score and floor\n",
    "    img = obs[25:195]  \n",
    "\n",
    "    # Downsize\n",
    "    img = img[::2, ::2]\n",
    "\n",
    "    # Take greyscale (black and white)\n",
    "    img = img.mean(axis=2)  \n",
    "\n",
    "    # color = np.array([210, 164, 74]).mean()\n",
    "    # img[img==color] = 0  \n",
    "    # img[img==144] = 0\n",
    "    # img[img==109] = 0\n",
    "    img[img != 0] = 1\n",
    "\n",
    "    # Is this needed? normalize the image from -1 to +1  \n",
    "    # No difference visually but tensor is different\n",
    "    if normalize:\n",
    "        img = (img - 128) / 128 - 1  \n",
    "\n",
    "    # print(\"before: \", obs.shape)\n",
    "    # print(\"after: \", img.shape)\n",
    "\n",
    "    # reshape to 1D tensor\n",
    "    return img.reshape(85,80,1)\n",
    "\n",
    "# frame stacking\n",
    "# https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\n",
    "# https://arxiv.org/pdf/1312.5602.pdf\n",
    "# need to get overlapping sets of frames\n",
    "# Ex: X1, X2, ... , X7 -> [X1, X2, X3, X4], [X2, X3, X4, X5], ... , [X4, X5, X6, X7]\n",
    "\n",
    "frame_skip = 4 # only one every four screenshot is considered. If there is no subsampling, not enough information to discern motion\n",
    "frame_stack_size = 4\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new):\n",
    "    frame = preprocess(state)\n",
    "    if is_new: # new episode\n",
    "        # replace stacked_frames with 4 copies of current frame\n",
    "        for i in range(frame_stack_size):\n",
    "            stacked_frames.append(frame)\n",
    "    else:\n",
    "        # take elementwise maxima of newest frame in stacked_frames and frame\n",
    "        stacked_frames.append(np.maximum(stacked_frames[3],frame))\n",
    "    stacked_state = np.stack(stacked_frames)\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fd50ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Training agent\n",
    "\n",
    "def train(env, replay_memory, model, target_model):\n",
    "\n",
    "    if len(replay_memory) <= REPLAY_SIZE:\n",
    "        return\n",
    "\n",
    "    batch = random.sample(replay_memory, BATCH_SIZE)\n",
    "    states = np.array([step[0] for step in batch])\n",
    "    q_values = model.predict(states)\n",
    "    succesive_states = np.array([step[3] for step in batch])\n",
    "    succesive_q_values = target_model.predict(succesive_states)\n",
    "\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "\n",
    "    for i, (state, action, reward, new_state, done) in enumerate(batch):\n",
    "        if not done:\n",
    "            # Bellman Equation : r(s) + gamma * max_a'(Q(s',a'))\n",
    "            qValue = reward + DISCOUNT_FACTOR * np.max(succesive_q_values[i])\n",
    "        else:\n",
    "            # Pick reward as the episode has ended; no succesive state\n",
    "            qValue = reward\n",
    "        \n",
    "        # TODO: Figure out y_train values work or not\n",
    "        # Temporal Difference\n",
    "        # q_value_arr for a state s : [qVal action1, qval action1, ..., qval action18] \n",
    "        q_value_arr = q_values[i]\n",
    "        # Qvalue for action a  : Q(s,a) + alpha(r(s) + gamma*max_a'(Q(s',a')) - Q(s, a))         \n",
    "        q_value_arr[action] = q_value_arr[action] + LEARNING_RATE * (qValue - q_value_arr[action])\n",
    "\n",
    "        X_train.append(state)\n",
    "        Y_train.append(q_value_arr)\n",
    "    \n",
    "    model.fit(np.array(X_train), np.array(Y_train), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62ea368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Deep Q-Learning agent\n",
    "\n",
    "epsilon = 1\n",
    "eps_min = 0.05\n",
    "eps_max = 1\n",
    "decay_steps = 500000\n",
    "\n",
    "model = network(state_shape, action_shape)\n",
    "target_model = network(state_shape, action_shape)\n",
    "target_model.set_weights(model.get_weights())\n",
    "\n",
    "# initialize with zeroes\n",
    "stacked_frames = deque(maxlen = frame_stack_size)\n",
    "for i in range(frame_stack_size):\n",
    "    stacked_frames.append([np.zeros((85,80), dtype=int)])\n",
    "\n",
    "# Memory buffer to store the last N experiences\n",
    "replay_memory = deque(maxlen=REPLAY_MEMORY)\n",
    "\n",
    "update_target_counter = 0\n",
    "step_counter = 0\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    score = 0 \n",
    "    done = False\n",
    "\n",
    "    stacked_state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "    while not done:\n",
    "        step_counter += 1\n",
    "\n",
    "        # Epsilon Greedy Strategy with explore probability epsilon\n",
    "        # Decay for epsilon (explore with atleast eps_min probability)\n",
    "        epsilon = max(eps_min, (eps_max - (eps_max-eps_min) * step_counter/decay_steps))\n",
    "\n",
    "        if np.random.rand() <= epsilon:\n",
    "            # Explore \n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Exploit best action from cnn\n",
    "            predictions = model.predict(np.array([stacked_state,])).flatten()\n",
    "            action = np.argmax(predictions)\n",
    "\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        new_stacked_state, stacked_frames = stack_frames(stacked_frames, new_state, False)\n",
    "\n",
    "        if step_counter % 4 == 0 or done:\n",
    "            replay_memory.append([stacked_state, action, reward, new_stacked_state, done])\n",
    "            train(env, replay_memory, model, target_model)\n",
    "\n",
    "        score += reward\n",
    "        stacked_state = new_stacked_state\n",
    "        state = new_state\n",
    "\n",
    "        if update_target_counter >= TARGET_MODEL_UPDATE:\n",
    "                update_target_counter = 0\n",
    "                target_model.set_weights(model.get_weights())\n",
    "\n",
    "        if done:\n",
    "              print('Score: {} after epsidoe = {}'.format(score, episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26b7dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f33118",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.save(\"models/model#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc61305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    env = gym.make('ALE/SpaceInvaders-v5', render_mode='human')\n",
    "    state = env.reset()\n",
    "\n",
    "    TEST_EPISODES = 100\n",
    "\n",
    "    model = keras.models.load_model(\"models/model1\")\n",
    "    scores = []\n",
    "    \n",
    "    # initialize with zeroes\n",
    "    stacked_frames = deque(maxlen = frame_stack_size)\n",
    "    for i in range(frame_stack_size):\n",
    "        stacked_frames.append([np.zeros((85,80), dtype=int)])\n",
    "\n",
    "    for episode in range(TEST_EPISODES):\n",
    "        state = env.reset()\n",
    "        score = 0 \n",
    "        done = False\n",
    "\n",
    "        stacked_state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "        while not done:\n",
    "            predictions = model.predict(np.array([stacked_state,])).flatten()\n",
    "            action = np.argmax(predictions)\n",
    "\n",
    "\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            new_stacked_state, stacked_frames = stack_frames(stacked_frames, new_state, False)\n",
    "\n",
    "            score += reward\n",
    "            stacked_state = new_stacked_state\n",
    "            state = new_state\n",
    "\n",
    "            if done:\n",
    "                scores.append(score)\n",
    "                print('episode: {}, score: {}'.format(episode, score))\n",
    "\n",
    "    \n",
    "    x = np.array([i for i in range(TEST_EPISODES)])\n",
    "    y = np.array(scores)\n",
    "\n",
    "    print(np.average(y))\n",
    "    \n",
    "    plt.plot(x, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1cdf7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b59615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
